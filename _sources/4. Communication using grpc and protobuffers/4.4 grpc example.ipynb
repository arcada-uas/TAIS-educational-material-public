{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Components for gRPC Communication\n",
    "\n",
    "With a foundational understanding of gRPC and Protocol Buffers, we can now proceed to configure the previously defined services to leverage gRPC for inter-component communication. It is important to note that while the AI on Demand platform will manage the communication between components during deployment, our responsibility lies in creating gRPC servers for each component and ensuring that the necessary functions are correctly implemented and operational.\n",
    "\n",
    "The following steps outline the process of preparing the services for gRPC communication:\n",
    "\n",
    "1. **Define the Service**\n",
    "2. **Write the Protofile**\n",
    "3. **Generate gRPC Code**\n",
    "4. **Implement the Server Using the Generated gRPC Code**\n",
    "5. **Create a Client Using the Generated gRPC Code to Test the Server**\n",
    "\n",
    "Since the services were defined in the previous chapter, we can proceed directly to writing the protofile. The service code is located in the `4. Communication using gRPC and Protocol Buffers\\example` directory of the GitHub repository. While this guide will demonstrate the process for the data component, the steps apply to all components once the underlying concepts are understood.\n",
    "\n",
    "## Creating the Protofile\n",
    "\n",
    "When defining the protofile, consider the data that needs to be exchanged between the components. For instance, in our pipeline, the first component must accept an empty message as input, a requirement when using the AI on Demand platform. Therefore, we need to define an empty message. Additionally, the component must return six distinct lists: the training and testing datasets for variables `x`, `y`, and `dates`. Consequently, we need to define a message type that can encapsulate these lists. Below is an example of how these messages might be defined:\n",
    "\n",
    "```proto\n",
    "syntax = \"proto3\";\n",
    "\n",
    "message Empty {}\n",
    "\n",
    "message CleanedData {\n",
    "    repeated double x_train = 1;\n",
    "    repeated double y_train = 2;\n",
    "    repeated double x_test = 3;\n",
    "    repeated double y_test = 4;\n",
    "    repeated string dates_train = 5;\n",
    "    repeated string dates_test = 6;\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, the repeated keyword is used to denote that a field contains a list.\n",
    "Once the messages are defined, we can proceed to define the service provided by our gRPC server. This involves specifying which RPC methods will be available on the server. In this case, we have only one function, clean_data, so we define our data service with a single RPC method, CleanData, which takes an Empty message as input and returns a CleanedData message. The following example illustrates this:\n",
    "\n",
    "```proto\n",
    "service DataService {\n",
    "    rpc CleanData (Empty) returns (CleanedData);\n",
    "}\n",
    "```\n",
    "The protofile should be saved under the name model.proto, as required by the AI on Demand platform. It is also essential to avoid defining a package within the protofile, as doing so may lead to errors during deployment. With the protofile written and saved, we can now proceed to generate the gRPC code.\n",
    "\n",
    "## Generating the gRPC Code\n",
    "\n",
    "With the protofile prepared, the next step is to generate the gRPC code. Ensure that the necessary gRPC tools are installed:\n",
    "\n",
    "```proto\n",
    "pip install grpcio-tools\n",
    "```\n",
    "\n",
    "You can generate the gRPC code from the protofile using the following command, executed in the directory containing the protofile:\n",
    "\n",
    "```proto\n",
    "python -m grpc_tools.protoc -I./ --python_out=. --grpc_python_out=. model.proto\n",
    "```\n",
    "This command will generate two files: model_pb2.py and model_pb2_grpc.py. The first file contains the message classes defined in the protofile, such as Empty and CleanedData, accessible via model_pb2.Empty and model_pb2.CleanedData. The second file contains the necessary code for creating the client and server for the microservice, which we will address in the next step.\n",
    "\n",
    "## Creating the Server\n",
    "\n",
    "Next, we will create the server for the component, leveraging the code generated in the gRPC files. The model_pb2_grpc.py file includes class definitions for both a servicer and a service, which are essential for server creation.\n",
    "\n",
    "The servicer is an abstract class that you implement to handle the server-side logic of your gRPC service. This is where you define the behavior of each RPC method by implementing the business logic described in your .proto file. To implement the server:\n",
    "\n",
    "1. Implement the Servicer: Create a subclass of the generated DataServiceServicer and implement the methods.\n",
    "2. Start the Server: Use the \n",
    "add_DataServiceServicer_to_server function to attach your servicer to the server and start it.\n",
    "\n",
    "Since the clean_data function has already been implemented in the service file, your task is to import it and, if necessary, add error handling to implement the CleanData RPC function. For simplicity, we will assume that the CSV file is located in the server's directory and will access it directly. Additionally, ensure that the return type of the RPC method matches the definition in the protofile. Specifically, the CleanedData message, which is returned by the CleanData RPC, must conform to the structure defined earlier. Below is an example of how to create the subclass for the servicer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from concurrent import futures\n",
    "import grpc\n",
    "import model_pb2_grpc\n",
    "import model_pb2\n",
    "from data_service import clean_data\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Output to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "class DataServiceServicer(model_pb2_grpc.DataServiceServicer):\n",
    "    def __init__(self):\n",
    "        self.dataset_filepath = 'uploaded_file.csv'\n",
    "\n",
    "    def CleanData(self, request, context):\n",
    "        logging.info(\"Cleaning data...\")\n",
    "        try:\n",
    "\n",
    "            if not os.path.isfile(self.dataset_filepath):\n",
    "                context.set_code(grpc.StatusCode.NOT_FOUND)\n",
    "                context.set_details(f\"Dataset file not found\")\n",
    "                return model_pb2.CleanedData()\n",
    "            \n",
    "            # Clean data\n",
    "            x_train, x_test, y_train, y_test, dates_train, dates_test = clean_data(self.dataset_filepath)\n",
    "\n",
    "            if len(x_train) != len(y_train):\n",
    "                raise ValueError(\"x_train and y_train have different lengths\")\n",
    "            \n",
    "            response = model_pb2.CleanedData(\n",
    "                x_train=x_train,\n",
    "                y_train=y_train,\n",
    "                x_test=x_test,\n",
    "                y_test=y_test,\n",
    "                dates_train=dates_train,\n",
    "                dates_test=dates_test\n",
    "            )\n",
    "            \n",
    "            # Return cleaned data along with the result of training\n",
    "            logging.info(\"Data cleaned successfully.\")\n",
    "            return response\n",
    "        \n",
    "        except Exception as e:\n",
    "            context.set_code(grpc.StatusCode.INTERNAL)\n",
    "            context.set_details(f\"Internal error: {str(e)}\")\n",
    "            return model_pb2.CleanedData()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function first verifies the availability of the dataset and subsequently utilizes it to invoke the previously defined clean_data function. Upon execution, the function returns the necessary values by the CleanedData message type specified in the protofile. Additionally, logging has been incorporated to facilitate debugging during testing.\n",
    "\n",
    "The next step in server setup is to initiate it. This involves using the generated add_DataServiceServicer_to_server function. Begin by creating a gRPC server with the following line of code:\n",
    "\n",
    "```python\n",
    "server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n",
    "```\n",
    "\n",
    "Here, futures.ThreadPoolExecutor(max_workers=10) configures a thread pool executor to manage concurrent RPCs. This setup allows the server to handle up to 10 concurrent requests (RPC calls) in parallel by managing a pool of threads for asynchronous task execution.\n",
    "\n",
    "Once the server is created, add the servicer with the following command:\n",
    "\n",
    "```python\n",
    "data_pb2_grpc.add_DataServiceServicer_to_server(DataServiceServicer(), server)\n",
    "```\n",
    "\n",
    "Finally, define a port, start the server, and ensure its continuous operation. The AI on Demand platform mandates that servers operate on port 8061. This configuration is achieved with:\n",
    "\n",
    "```python\n",
    "server.add_insecure_port('[::]:8061')\n",
    "server.start()\n",
    "server.wait_for_termination()\n",
    "``` \n",
    "\n",
    "You now possess all the essential components for your server. For the complete server code, refer to the data_service_server.py file located in the directory 4. Communication using gRPC and Protocol Buffers\\example\\data in the GitHub repository.\n",
    "\n",
    "## Creating the Client for Testing\n",
    "While creating a client is not strictly necessary for deployment, it is highly recommended to test the server to ensure proper functionality.\n",
    "\n",
    "Creating a gRPC client involves establishing a communication channel, creating a stub to interact with the server, making a request, and handling the response. The stub, generated from the protofile, serves as an intermediary between the client and server. It provides methods corresponding to the RPCs defined in the .proto file, enabling the client to invoke these methods as if they were local functions, despite being executed on a remote server.\n",
    "\n",
    "To set up the client:\n",
    "\n",
    "1. **Create a Channel**: Establish a communication path to the server by specifying its address (localhost:8061 in this instance):\n",
    "\n",
    "    ```python\n",
    "    with grpc.insecure_channel('localhost:8061') as channel:\n",
    "    ```\n",
    "    The with statement ensures that the channel is properly closed upon completion of the operation.\n",
    "\n",
    "2. **Define the Stub**: Use the channel to instantiate a stub:\n",
    "\n",
    "    ```python\n",
    "    stub = model_pb2_grpc.DataServiceStub(channel)\n",
    "    ```\n",
    "\n",
    "3. **Send a Request**: Define an empty message and invoke the CleanData method using the stub:\n",
    "\n",
    "    ```python\n",
    "    empty_message = model_pb2.Empty()\n",
    "                \n",
    "    # Call the CleanData method\n",
    "    response = stub.CleanData(empty_message)\n",
    "    ```\n",
    "4. **Process the Response**: Evaluate the server’s response and handle it as needed. This may involve printing the response data or using it for further computations. To test the next component, save the results as follows:\n",
    "\n",
    "    ```python\n",
    "    if response.x_train and response.x_test and response.y_train and response.y_test and response.dates_train and response.dates_test:\n",
    "                    print(\"x_trian:\", response.x_train)\n",
    "                    print(\"x_test:\", response.x_test)\n",
    "                    print(\"y_train:\", response.y_train)\n",
    "                    print(\"y_test:\", response.y_test)\n",
    "                    print(\"Dates Train:\", response.dates_train)\n",
    "                    print(\"Dates Test:\", response.dates_test)\n",
    "\n",
    "                    # save the data to a file for later usage\n",
    "                    with open('cleaned_data.pkl', 'wb') as f:\n",
    "    pickle.dump(response, f)\n",
    "    logging.info(\"Cleaned data saved to cleaned_data.pkl.\")\n",
    "    ```\n",
    "\n",
    "These steps complete the client setup. For the full client code, refer to the data_client.py file in the same directory as the data server. To test the server, execute python data_service_server.py in one terminal and python data_client.py in another. The cleaned data should be printed, and a new .pkl file containing the results will be generated in the same directory as the server and client.\n",
    "\n",
    "## Defining Additional Servers\n",
    "\n",
    "You may proceed by similarly creating the other servers:\n",
    "1. Define the service\n",
    "2. Write the protofile\n",
    "3. Generate gRPC code\n",
    "4. Create the server\n",
    "5. Create and test the client\n",
    "\n",
    "The gRPC code for training and testing, including all corresponding clients, has already been implemented and can be found in the repository under the directory TAIS-educational-material-public\\4. communication using grpc and protobuffers\\example.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "You are now acquainted with the process of creating a gRPC server and testing it using a client. The next phase involves developing web applications for the data and testing components to enable users to upload a CSV file and view the test results. To fully grasp the forthcoming chapter, ensure you review the code from this chapter, particularly the testing server, as it will be central to the next phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
