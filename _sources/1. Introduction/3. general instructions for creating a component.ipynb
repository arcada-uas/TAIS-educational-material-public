{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a component\n",
    "\n",
    "We will soon move onto the provided example on how to create a pipeline, but before we do so, it is important to have a good overview of the necessary steps. In this file we will briefly explain the steps necessary to create a component. The goal is to create a clear overview of the entire process, and therefor we will not go in depth into exactly what each step entails in practicality, but rather the reason behind the step and the outcome. \n",
    "\n",
    "In order to create a component which could be used in a ML pipeline you will need to write the necessary functions in a service file, create a web application, create a proto file based on the services provided, generate the gRPC code and implement the client and the server. All of this should then be containerized and the container image (Docker image) can then be uploaded to dockerhub. From there you can upload the dockerimage together with the protofile to the AI builder platform where it can be used as part of a pipeline. We will now go more in depth into how to complete these steps. \n",
    "\n",
    "In these steps it is assumed that you already have identified your problem, objectives and deliverable items, as well as planned what components to create and what their input and outputs are, and you're now ready to start coding.\n",
    "\n",
    "\n",
    "### 1. Writing the service. \n",
    "\n",
    "First you should know what kind of service you want or need. This could be for example data filtering, model training, making predictions etc. Once you have decided on the service you can pick any language supported by protobuffers that's suitable to implement the necessary functions. Protobuffers support C++, C#, Dart, Go, Java, Kotlin, Objective-C, Python and Ruby. The service should consists of different functions that take some input and produce an output. These functions will then later be called once we implement the web application. \n",
    "\n",
    "One example of a service in this case could be a function clean_data that would take some raw data and reformat it so that it is in a fitting format for the next component. Now you might notice why it is so important to define the inputs and outputs of all the components before starting with the coding, since all components are somewhat dependent on other components. \n",
    "\n",
    "### 2. Implement the web applications for testing\n",
    "\n",
    "Once you have defined your services, it is time to implement the web application for the component in order to test it's functionality before moving forward. The webapplication can be written using any suitable library, but we have chosen to use Flask as it is an easy and lightwight library for web appliation development. \n",
    "\n",
    "### 3. Local testing\n",
    "\n",
    "Before comtinuing it is important that you test your application. This way you can ensure that the application works as expected and can confidently move on to the next step.\n",
    "\n",
    "At this stage, testing can be done either through command line, through the webpage where the application is run, or using postman or any other similar service. We will later go more in depth into how to do this.\n",
    "\n",
    "### 4. Make a proto file\n",
    "\n",
    "Once you have tested your web application and assured that everything works as expected, you're ready to define the protofile based on the services. This will also later be explained in more detail, but as a general idea you need to take into consideration the data that is necessary in order to use the functions you have implemented. You should define the necessary message structures as well as the service that will be provided according to the instructions in the protocol buffers notebook. \n",
    "\n",
    "### 5. Generate gRPC code\n",
    "\n",
    "Once you have defined the protofile you can use it to generate the necessary gRPC code for your service. The process of generating the code as well as the resulting code will differ depending on the language you have chosen for your application. As mentioned in the gRPC notebook later on, the generated code will contain code for the service interface as well as the grpc-client and -server.\n",
    "\n",
    "### 6. Client and server\n",
    "\n",
    "Next you will need to implement the grpc-client and -server using the generated gRPC code. The server should implement the services defined in the protofile. Since you already defined them when creating the service, you can simply import the functions. The client should have a stub which should've been generated in the previous step. The stub allows for easy communication between the client and the server. \n",
    "\n",
    "### 7. Testing the grpc-server\n",
    " \n",
    "Again, it is important that you test your application before continuing with the next step. You can test the grpc-server by running it using the command python server.py where server.py is the name of the file containing your grpc server, and also running the client with the same command using the name of the file where the client code is located. The client should call the different methods available at the server. This way you can test if all the functions on the server side works as expected, and if the communication also works properly.\n",
    "\n",
    "### 8. License\n",
    "\n",
    "Before creating the docker image you will need to include an apache 2.0 license as gRPC falls under it.\n",
    "\n",
    "### 9. Prepare the docker file\n",
    "\n",
    "Next you can create the dockerfile. You will need to copy over the necessary code, run installation commands and write an entrypoint command. After having written the dockerfile you can build the docker image with the docker command \"docker build -t [name of build] .\" in the folder containing the code and the dockerfile. Once the docker image has been build you can run it either through the command line or using docker desktop. \n",
    "\n",
    "When you have multiple components that are expected to communicate together it's a good idea to also define a docker-compose.yaml file. This file will define the dependencies between the applications. This means that if for example the model training component is dependent on the output of the data cleaning component, docker will ensure that the data cleaning component is up an running before starting the model training component. Docker also makes sure that the components are in the same network so that they can communicate between each other. \n",
    "\n",
    "### 10. Final testing of the component\n",
    "\n",
    "Before moving on to uploading the component to the AI builder platform, it's a good idea to test that the application works as expected also after the containerization. Run the application using docker to test the functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
