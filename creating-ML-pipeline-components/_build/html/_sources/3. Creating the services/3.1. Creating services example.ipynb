{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Services and Testing\n",
    "\n",
    "Having established the pipeline structure, identified its components, and delineated the inputs and outputs for each, we can now proceed with the implementation of these components. As outlined in Chapter 1.5 general guidelines for creating a component, the initial step involves defining the services. By \"services\" we refer to the functions that will be deployed on the servers. These services are defined independently of the servers to ensure that the core functionality is thoroughly tested before further development.\n",
    "\n",
    "## Data Component\n",
    "\n",
    "We begin with the data collection and cleaning component, hereafter referred to as the data component. The primary objective of this component is to clean raw data from a CSV file. As previously mentioned, this CSV file will be sourced from the web application. In defining the service, our focus will remain on the core functionality rather than on the specifics of how variables will be received by the server. The service function is designed to accept a CSV file, clean and structure the data, and return the processed results. A foundational code for this service is already available in the `stock_price_prediction` notebook. The code for the data cleaning and structuring has simply been turned into a sepatate function. All lists have also been flattened before being returned. This has been done to simplify the communication between the components later on. Below you can see an example of the service: \n",
    "\n",
    "\n",
    "```python\n",
    "def clean_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data['Previous_Close'] = data['Close'].shift(1)\n",
    "    data = data.dropna()\n",
    "    \n",
    "    x = data[['Previous_Close']]\n",
    "    y = data['Close']\n",
    "    dates = data['Date']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test, dates_train, dates_test = train_test_split(\n",
    "        x, y, dates, test_size=0.2, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Flatten the lists\n",
    "    x_train_flat = [item for sublist in x_train.values for item in sublist]\n",
    "    x_test_flat = [item for sublist in x_test.values for item in sublist]\n",
    "    y_train_flat = y_train.tolist()\n",
    "    y_test_flat = y_test.tolist()\n",
    "    dates_train_str = dates_train.dt.strftime('%Y-%m-%d').tolist()\n",
    "    dates_test_str = dates_test.dt.strftime('%Y-%m-%d').tolist()\n",
    "    \n",
    "    return x_train_flat, x_test_flat, y_train_flat, y_test_flat, dates_train_str, dates_test_str\n",
    "```\n",
    "This function processes a CSV file by converting the 'Date' column into the appropriate format. It creates a new feature, `Previous_Close`, based on the prior day's closing value and removes any rows containing NaN values. The function then splits the data into training and testing sets using `train_test_split()`, and finally, flattens the resulting lists before returning them.\n",
    "\n",
    "### Testing the Service\n",
    "\n",
    "To verify the functionality of this service, it is recommended to write a simple test function that invokes the `clean_data` function. A basic test function is provided in the `data_test.py` file. This function specifies the CSV file to be used as input, calls `clean_data`, and prints the resulting values. The results are also saved to a JSON file for subsequent use when testing the next component, namely the training component. The CSV file used for testing is a shortened version of the one employed in the stock price prediction pipeline, which simplifies the testing process by reducing the volume of data. The `test_data_service` function is defined as follows:\n",
    "\n",
    "```python\n",
    "def test_data_service():\n",
    "    csv_file = 'MSFT.US.test.csv'\n",
    "    returned_data = clean_data(csv_file)\n",
    "    print(returned_data)\n",
    "\n",
    "    #save the data to a json file\n",
    "    with open('cleaned_data.json', 'w') as f:\n",
    "        #add data with variable names to json file:\n",
    "        json.dump({\n",
    "            'x_train': list(returned_data[0]),\n",
    "            'x_test': list(returned_data[1]),\n",
    "            'y_train': list(returned_data[2]),\n",
    "            'y_test': list(returned_data[3]),\n",
    "            'dates_train': list(returned_data[4]),\n",
    "            'dates_test': list(returned_data[5])\n",
    "        }, f)\n",
    "    return\n",
    "```\n",
    "\n",
    "Ensure that this function is invoked in the file, and then execute `python data_test.py` to observe the output generated by the `clean_data` function. Should the service fail to function as expected, you may utilize the built-in debugger in VS Code to diagnose and rectify the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training Component\n",
    "\n",
    "We will now apply a similar approach to the training component. The service for this component is intended to receive the cleaned data, train a linear regression model, and return the trained model. It is important to note that the `x_train` list was flattened in the data component to streamline later steps (particularly during the implementation of gRPC). However, this list must be converted back into a 2D array, which necessitates the following line of code:\n",
    "\n",
    "```python\n",
    "x_train = np.array(x_train).reshape(-1, 1)\n",
    "```\n",
    "Apart from this adjustment, the existing code from the `stock_price_prediction` notebook can be utilized. Additionally, consideration must be given to how the trained model will be transmitted to the subsequent component. To facilitate this, the model will be serialized into a binary format. The `train_model` function is implemented as follows:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pickle\n",
    "\n",
    "def train_model(x_train, y_train):\n",
    "    model = LinearRegression()\n",
    "    x_train = np.array(x_train).reshape(-1, 1)\n",
    "    model.fit(x_train, y_train)\n",
    "    model_binary = pickle.dumps(model)\n",
    "\n",
    "    return model_binary\n",
    "\n",
    "```\n",
    "## Testing the Service\n",
    "\n",
    "As with the previous component, it is imperative to test the service by writing a test function that invokes the `train_model` function. For this purpose, we will use the JSON file generated during the testing of the data component to extract the cleaned data. This time, the test function will also save the resulting model for use in testing the final component. Since the model is serialized into a binary format, it will be saved accordingly. The test function is implemented as follows:\n",
    "\n",
    "```python\n",
    "def test_train_model():\n",
    "    # Read the JSON file\n",
    "    with open('cleaned_data.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract x_train and y_train values\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "\n",
    "    # Call the train_model function\n",
    "    model_binary = train_model(x_train, y_train)\n",
    "\n",
    "    with open('model.pkl', 'wb') as f:\n",
    "        f.write(model_binary)\n",
    "    \n",
    "\n",
    "    # Print the result\n",
    "    print(\"Model trained and serialized successfully.\")\n",
    "    print(f\"Serialized model size: {len(model_binary)} bytes\")\n",
    "```\n",
    "Ensure that the JSON file created during the data cleaning process is moved to the directory containing the training component before executing the file with the testing function.\n",
    "\n",
    "## Model Testing Component\n",
    "\n",
    "The service for the model testing component must be capable of handling multiple datasets: `x_train`, `y_train`, `dates_train`, `x_test`, `y_test`, and `dates_test`, as well as the trained model. Based on the `stock_price_prediction` notebook, we have an outline for constructing this service. The service will generate predictions, calculate the RMSE (Root Mean Square Error), and produce a plot of the results. The key difference in this context is that the plot must be returned in a format suitable for transmission between components. We have opted to encode the plot in a binary format, and it will also be saved for easy visualization. The `test_model` function is implemented as follows:\n",
    "\n",
    "```python \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "def test_model(model, x_test, y_test, dates_test):\n",
    "    x_test = np.array(x_test).reshape(-1, 1)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(f\"x_test in testing service: {x_test}\")\n",
    "    print(f\"y_pred in testing service: {y_pred}\")\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "\n",
    "    # Create a BytesIO object to save the plot in-memory\n",
    "    plot_stream = io.BytesIO()\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(dates_test, y_test, label='Actual')\n",
    "    plt.plot(dates_test, y_pred, label='Predicted')\n",
    "\n",
    "    # Format the date on the x-axis\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=120))  # Set major ticks every 120 days\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "    plt.gcf().autofmt_xdate()  # Rotate date labels vertically\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.title('MSFT Stock Price Prediction')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot to the BytesIO object\n",
    "    plt.savefig(plot_stream, format='png')\n",
    "    plt.close()\n",
    "\n",
    "    # Get the binary data from the BytesIO object\n",
    "    plot_stream.seek(0)\n",
    "    plot_binary = plot_stream.read()\n",
    "\n",
    "    return rmse, plot_binary\n",
    "\n",
    "```\n",
    "This function ensures that `x_test` is reshaped into a 2D array before predictions are made using the model's `predict` function. The RMSE is calculated using the predicted and actual `y_test` values, providing a metric of the model's accuracy. A plot is then generated to visually compare the predicted values with the actual ones. This plot is saved and returned along with the RMSE value and the binary-encoded plot.\n",
    "\n",
    "### Testing the service\n",
    "\n",
    "To verify the functionality of this service, a test function should be written. This function will utilize the cleaned data saved in the JSON file and the model saved as a binary file during the training component. The test function will then call the `test_model` function. The implementation is as follows:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from test_service import test_model\n",
    "\n",
    "def test_test_model():\n",
    "    # Load the model from the pickle file\n",
    "    with open('model.pkl', 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "\n",
    "    # Load the test data from the JSON file\n",
    "    with open('cleaned_data.json', 'r') as file:\n",
    "        test_data = json.load(file)\n",
    "\n",
    "    # Extract and convert the test data\n",
    "    x_test = np.array(test_data['x_test'])\n",
    "    y_test = np.array(test_data['y_test'])\n",
    "    dates_test = [datetime.strptime(date, '%Y-%m-%d') for date in test_data['dates_test']]\n",
    "\n",
    "    # Call the test_model function\n",
    "    test_model(model, x_test, y_test, dates_test)\n",
    "\n",
    "    # Check the output (this can be more sophisticated with assertions)\n",
    "    print(\"Test completed successfully.\")\n",
    "\n",
    "# Run the test function\n",
    "if __name__ == \"__main__\":\n",
    "    test_test_model()\n",
    "```\n",
    "\n",
    "Before running this file, ensure that the JSON and pickle files are moved to the directory containing the test file.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have now successfully developed and tested the three services necessary for our components. Having confirmed that these functions perform as intended, we can proceed to the next stage with confidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
