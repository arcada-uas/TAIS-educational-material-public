{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This material aims at providing a straight forward and easy way to learn how to create machine learning pipelines consisting of different components, or micro services, and deploying such pipelines using the AI builder tools. We want to provide a comprehensive guide especially for creating such components so that more people can effectivly utilize the services and tools provided by AI builder. This will be done through materials on the necessary technologies and tools, combined with an example with step-by-step instructions and additional information. \n",
    "\n",
    "To get a better general idea of how the end product should work and the structure of it, let's first look at the basic idea of a pipeline consisting of components. \n",
    "\n",
    "A machine learning pipeline is something that automates the workflow it takes to produce a machine learning model. Such a pipeline ususally include specific steps, such as data collection and labeling, model training and evaluation as well as deployment of the model. We want to split these steps into different microservices, or components. Each component should be responsible for one step of the pipeline. When the components are connected together, you would get the full pipeline which would create one specific ML model. \n",
    "\n",
    "![Simple Pipeline Architecture](./pic1.1.1.png)\n",
    "\n",
    "\n",
    "In the graph above you can see a diagram of a basic pipeline. Notice how the output from one component becomes the input for the next, like for example the cleaned data which is the output of the data collection component, also is the input for the next component, the data labeling component. This means that there needs to be communication between the different components. We will cover how the communication will work later, but the basic idea is that each component is a web application and they communicate with each other using grpc and protobuffers. \n",
    "\n",
    "The idea behind splitting the pipeline into different components is to then create a library of components which could be connected in different ways to create different pipelines and therefor models. This can be done using the AI builder platform. Once the components have been created they can be uploaded to the AI builder platform where they can be connected together to create the pipeline. Once they are connected, you can download a deployable pipeline contained in a solution.zip file. Using the playground app provided as part of the AI builder project, you can deploy the pipeline and run the components. Below you can see a graph outlining the process of creating the pipeline.\n",
    "\n",
    "![creation of pipeline](./pic1.1.2.png)\n",
    "\n",
    "Following these steps and creating a pipeline requires knowledge of quite a few technologies and tools. We have compiled a list of the technologies and tools we will be using in the example provided in this material. Those tools and technologies include the following:\n",
    "\n",
    "- Python\n",
    "- Javascript \n",
    "- Virtual Environments\n",
    "- Machine Learning\n",
    "- Flask\n",
    "- HTML\n",
    "- CSS\n",
    "- **Protocol Buffers**\n",
    "- **gRPC**\n",
    "- **Containerization**\n",
    "- **Docker**\n",
    "- Microservices\n",
    "- SSL\n",
    "- Postman\n",
    "- Highcharts\n",
    "- **Kubernetes**\n",
    "- Rahti\n",
    "\n",
    "\n",
    "To better understand how to build the components that make up pipelines, we will give some basic insights into the bolded required technologies and how they work. We will also provide an examle with step by step instructions where we can provide an example of how to use the other technologies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
