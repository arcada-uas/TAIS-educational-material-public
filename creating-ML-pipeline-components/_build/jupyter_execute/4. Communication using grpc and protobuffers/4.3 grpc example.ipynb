{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the components to communicate using gRPC\n",
    "\n",
    "Now that we have a bit more knowledge about gRPC and protobuffers, we can configure the services we have defined previously to utilize grpc for communication. The steps necessary to take before you can use grpc for communication are the following:\n",
    "\n",
    "1) Define the service\n",
    "2) write the protofile\n",
    "3) generate gRPC code\n",
    "4) Implement client and server using the generated gRPC code\n",
    "\n",
    "We have already defined the services in the previous chapter, so we can move onto writing the protofile. Let's cop over the service files from the previous chapter! You can find the service code in the sub folders inside the example folder. \n",
    "\n",
    "## Creating the protofile\n",
    "\n",
    "To define the protofile we need to consider what data will be passed between the server and the client. We want the server to receive a csv file, and this can be done in a few different ways, either as a string or as binary data. For this example we have chosen binary data, so we know that we need one message type for this. The function returns six different lists, so we also need to define a message type that can hold the lists. Here is an example of how to define these messages:\n",
    "\n",
    "```proto\n",
    "syntax = \"proto3\";\n",
    "\n",
    "package data;\n",
    "\n",
    "// Request message containing CSV data as bytes.\n",
    "message DataRequest {\n",
    "    bytes csv_content = 1;\n",
    "}\n",
    "\n",
    "\n",
    "// Response message containing cleaned data.\n",
    "message DataResponse {\n",
    "    repeated double x_train = 1;\n",
    "    repeated double x_test = 2;\n",
    "    repeated double y_train = 3;\n",
    "    repeated double y_test = 4;\n",
    "    repeated string dates_train = 5;\n",
    "    repeated string dates_test = 6;\n",
    "}\n",
    "\n",
    "\n",
    "// The DataService definition.\n",
    "service DataService {\n",
    "    // Sends CSV content and receives cleaned data.\n",
    "    rpc CleanData (DataRequest) returns (DataResponse);\n",
    "}\n",
    "```\n",
    "Here you can see that the keyword \"repeated\" has been used when the datatype is a list. We have also defined the service that will be provided and it's functions, which in this case is only the CleanData function. As input and output we use the defined messages.\n",
    "\n",
    "## Generating the grpc code\n",
    "\n",
    "Now we have all the necessary components for the protofile. Next we need to generate the grpc code from the protofile. This can be done in the folder containing the protofile with the following command:\n",
    "\n",
    "```bash\n",
    "python -m grpc_tools.protoc -I./ --python_out=. --grpc_python_out=. data.proto\n",
    "```\n",
    "This should generate two files with code that you can use for defining the server and the client. The files should have the suffixes _pb2.py and _pb2_grpc.py. The first file contains message classes for the messages you defined in the protofile. There are two messageclasses, as we have two messages, and those can be accessed with data_pb2.DataRequest and data_pb2.DataResponse. The second file contains the necessary code for creating the client and the server for the microservice, which is the next step. \n",
    "\n",
    "## Creating the server\n",
    "Let's first create the server for the microservice. For this you are going to want to utilize some of the code from the generated grpc files. In the file ending with _pb2_grpc.py you should find class definitions for both a servicer and a service, which you are going to need when creating the server. \n",
    "\n",
    "The Servicer is a class that you implement to handle the server-side logic of your gRPC service. This is where you define how each RPC method should behave by writing the actual business logic for the methods described in your .proto file. You need to create a subclass of the generated DataServiceServicer and implement the RPC methods defined in the .proto file. The Service class is used internally by gRPC to provide methods to interact with the RPCs. It's auto-generated by protoc and typically includes static methods for the client to call RPCs and functions to add handlers to the server. The Service is not typically modified directly.\n",
    "\n",
    "To implement the server you should follow these steps:\n",
    "1) Implement the Servicer: Create a subclass of the generated DataServiceServicer and implement the methods.\n",
    "2) Start the Server: Use the add_DataServiceServicer_to_server function to attach your servicer to the server and start it.\n",
    "\n",
    "Since we already wrote the function clean_data in the service file, all you have to do is import it and possibly add some error handling to implement the RPC function CleanData. You also need to make sure that the return type of the RPC method follows the definition in the protofile. We defined the return type to be a collection of three values, previous_close, close and dates, so you need to make sure that the returned value follows this definiton. Here is an example of how to create the subclass for the servicer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# data_service_server.py\n",
    "from concurrent import futures\n",
    "import grpc\n",
    "from data import data_pb2_grpc\n",
    "from data import data_pb2\n",
    "import pandas as pd\n",
    "from data.data_service import clean_data\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "\n",
    "class DataServiceServicer(data_pb2_grpc.DataServiceServicer):\n",
    "    def CleanData(self, request, context):\n",
    "        try:\n",
    "            # Read CSV content from the request\n",
    "            csv_content = request.csv_content\n",
    "            csv_file = io.BytesIO(csv_content)\n",
    "            \n",
    "            logging.info(\"Received CSV data, cleaning...\")\n",
    "            # Clean data\n",
    "            x_train, x_test, y_train, y_test, dates_train, dates_test = clean_data(csv_file)\n",
    "            logging.info(\"Data cleaned successfully\")\n",
    "\n",
    "            print(x_train, x_test, y_train, y_test, dates_train, dates_test)\n",
    "\n",
    "            \n",
    "            return data_pb2.DataResponse(\n",
    "                x_train=x_train,\n",
    "                x_test=x_test,\n",
    "                y_train=y_train,\n",
    "                y_test=y_test,\n",
    "                dates_train=dates_train,\n",
    "                dates_test=dates_test\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Error cleaning data\")\n",
    "            context.set_code(grpc.StatusCode.INTERNAL)\n",
    "            context.set_details(f\"Internal error: {str(e)}\")\n",
    "            return data_pb2.DataResponse()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that the code does essentially the same things as the code for the data app, at the route /get_clean_data. We first read the csv content and save it to a csv file. The file can then be passed to the clean_data function that we wrote earlier. Only difference here is the way we receive and return the data. For the grpc server, we make sure to save the values in the correct return format, data_pb2.DataResponse. \n",
    "\n",
    "Now we need to complete the second step for creating the server, start the server. For this we can use the generated function add_DataServiceServer. In order to do this we need to first create a grpc server, that can be done using the following line:\n",
    "\n",
    "```python\n",
    "server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n",
    "```\n",
    "futures.ThreadPoolExecutor(max_workers=10) specifies the thread pool executor for handling concurrent RPCs. ThreadPoolExecutor manages a pool of threads for executing tasks asynchronously. Here, it allows the server to handle up to 10 concurrent requests (RPC calls) in parallel.\n",
    "\n",
    "Now that we have a server, we can add the servicer to it using the following:\n",
    "\n",
    "```python\n",
    "data_pb2_grpc.add_DataServiceServicer_to_server(DataServiceServicer(), server)\n",
    "```\n",
    "\n",
    "Lastly we need to define a port, start the server and make sure it stays running. This can be done like this:\n",
    "\n",
    "```python\n",
    "server.add_insecure_port('[::]:50051')\n",
    "server.start()\n",
    "server.wait_for_termination()\n",
    "``` \n",
    "Now you have all the necessary components for your server. You can take a look at the data_service_server file to see the final code for the server. This file can be found in the example folder.\n",
    "\n",
    "## Creating the client\n",
    "\n",
    "Lastly we need to create a client in order to interact with our server. Creating a gRPC client involves setting up a communication channel, creating a stub to interact with the server, making a request, and handling the response. You are going to want to use the stub that was generated from the protofile. The stub acts as an intermediary between your application code and the remote gRPC service. The stub provides methods that correspond to the RPCs defined in the .proto file, allowing your application to invoke these methods as if they were local functions, even though they are executed on a remote server. \n",
    "\n",
    "First we are going to create a channel, a communication path, to the server. You need to specify the server's address (in this case, localhost:50051) in order to create the channel. This can be done like so:\n",
    "\n",
    "```python\n",
    "with grpc.insecure_channel('localhost:50051') as channel:\n",
    "```\n",
    "The with statement ensures the channel is properly closed when the operation is done.\n",
    "\n",
    "Next we will define the stub using the channel:\n",
    "\n",
    "```python\n",
    "stub = data_pb2_grpc.DataServiceStub(channel)\n",
    "```\n",
    "\n",
    "Now let's read and prepare the data for the request. Reading the file as bytes prepares the data to be sent in the DataRequest as defined in the protofile. This is how the reading can be done:\n",
    "\n",
    "```python\n",
    "csv_file_path = './MSFT.US.csv'\n",
    "\n",
    "with open(csv_file_path, 'rb') as f:\n",
    "    csv_content = f.read()\n",
    "```\n",
    "\n",
    "Next we need to construct a request message using the data read in the previous step. The request message format is defined in the .proto file. \n",
    "\n",
    "```python\n",
    "request = data_pb2.DataRequest(csv_content=csv_content)\n",
    "```\n",
    "Now we can invoke the remote method on the stub. This sends the request to the server and waits for a response.\n",
    "\n",
    "```python\n",
    "response = stub.CleanData(request)\n",
    "```\n",
    "Now we can check the server’s response and process it as needed. This could involve printing the response data or using it in further computations.\n",
    "\n",
    "```python\n",
    "if response.x_train and response.x_test and response.y_train and response.y_test and response.dates_train and response.dates_test:\n",
    "    print(\"x_trian:\", response.x_train)\n",
    "    print(\"x_test:\", response.x_test)\n",
    "    print(\"y_train:\", response.y_train)\n",
    "    print(\"y_test:\", response.y_test)\n",
    "    print(\"Dates Train:\", response.dates_train)\n",
    "    print(\"Dates Test:\", response.dates_test)\n",
    "else:\n",
    "    print(\"No data returned or some fields are empty.\")\n",
    "```\n",
    "\n",
    "These are all the necessary parts of the client. You can see the entire client file (data_client.py) in the \"1.4connecting_services_with_grpc\" folder as well. \n",
    "\n",
    "\n",
    "## Testing\n",
    "Now that you have all the necessary files, you can test the server and client. You can run the server with the command python3 data_service_server.py and the same goes for the client. You should see the cleaned data being printed on the client side. \n",
    "\n",
    "Now you could continue by creating the other services in the same way:\n",
    "\n",
    "1) define the service\n",
    "2) write protofile\n",
    "3) generate grpc code\n",
    "4) create server and client\n",
    "\n",
    "To test the services individually, write individual client files. If you want to test them together, you can create a common client file which uses the stubs of all the different services to communicate. We have already implemented the grpc code for the training and testing, and included all the clients into one file called client.py. If you run all the servers and then run the client you should be able to see how the whole pipeline is executed and a graph is produced as output. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}